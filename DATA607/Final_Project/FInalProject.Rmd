---
title: "Final Project"
author: "Devin Teran, Gabe Abreu, Amit Kapoor, Subhalaxmi Rout"
date: "4/27/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(twitteR)
library(quantmod)
library(rvest)
library(stringr)
library(purrr)
library(tidytext)
library(dplyr)
library(tidyr)
library(ggplot2)
```

```{r get_Twitter}
consumer_key <- "Lsk1wfKakbL6rfFC1dcLfpSrb"
consumer_secret <- "rKywBX5uJ3d13drP8NxgWGxC3TtPraKZiucS58J2OzFW0Rpcci"
access_token <- "1257449305616650243-J8E4TpO0Fmu6v5KXyY8T9owgSsPI7v"
access_secret <- "tgCzUd7fXgnSZFrLvmeMBYNJzH3i1hZkDX16PK3APkM3E"

#now lets connect
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)

# We can request only 3200 tweets
trump_tweets <- twitteR::searchTwitter("realDonaldTrump", n = 3200, since = "2020-03-16")
trump_tweets_df <- tbl_df(purrr::map_df(trump_tweets, as.data.frame))
tweets <- trump_tweets_df %>%
  select(id, statusSource, text, created)
DT::datatable(tweets)

```

```{r get_urls}
base_url <- 'https://www.whitehouse.gov/briefings-statements/remarks-president-trump-vice-president-pence-members-coronavirus-task-force-press-briefing-'

getPageURLs <- function(url) {
   add_number <- seq(2,33)
   urls <- str_c(base_url, add_number)
   return(urls)
}

urls <- getPageURLs(urls)
```

```{r get-data}
wh_briefings <- purrr::map(urls, ~read_html(.x) %>% html_nodes("p") %>% html_text())
wh_dates <- purrr::map(urls, ~read_html(.x) %>% html_nodes("p") %>% html_node("time") %>% html_text())
 
```


```{r clean-dates}
testFrame <- data.frame(date = character(),
                        stringsAsFactors = FALSE)


for (i in 1:length(wh_dates)){
  testFrame <- rbind(testFrame,cbind(as.data.frame(unlist(wh_dates[[i]]),stringsAsFactors = FALSE),i))
}

dateFrame <- na.omit(testFrame)

```

```{r list-to-dataframe}
data0 <- data.frame(text=character(),
                   Day=integer(),
                 stringsAsFactors=FALSE)

for (i in 1:length(wh_briefings)){
  data0 <- rbind(data0,cbind(as.data.frame(unlist(wh_briefings[[i]]),stringsAsFactors = FALSE),i))
}

colnames(data0) <- c('text','day')
```

```{r}

correctMatch <- inner_join(dateFrame,data0,by= c("i" = "day"))
colnames(correctMatch) <- c('date', 'day', 'text')

data <- correctMatch

```

```{r create-stop-word-list}
custom_stop_words <- bind_rows(tibble(word = c("EDT"), 
                                      lexicon = c("custom")), 
                               stop_words)

custom_stop_words
```
```{r clean-data}
tidy_data <- data %>%
  mutate(linenumber = row_number()) %>%
  ungroup() %>%
  unnest_tokens(word, text)
```



```{r apply lexicon analysis}
afinn <- get_sentiments("afinn") 

tidy_data %>%
  anti_join(stop_words) %>%
  group_by(day) %>%
  inner_join(afinn) %>%
  count(word, sort = TRUE) %>%
  arrange(day,-n)
```



```{r affin, echo=TRUE}
affinLex <- get_sentiments("afinn")
wf.affin <- tidy_data %>% 
  anti_join(stop_words) %>% 
  group_by(day) %>%
  inner_join(affinLex) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method="affin")

wf.affin
```


```{r bing, echo=TRUE}
bingLex <- get_sentiments("bing")
wf.bing <- tidy_data %>% 
  anti_join(stop_words) %>% 
  group_by(day) %>%
  inner_join(bingLex) %>% 
  count(day, sentiment) %>% 
  spread(sentiment, n, fill=0) %>% 
  mutate(sentiment = positive - negative) %>%
  mutate(method="bing")

wf.bing
```

```{r nrc, echo=TRUE}
nrcLex <- get_sentiments("nrc")
wf.nrc <- tidy_data %>% 
  anti_join(stop_words) %>% 
  group_by(day) %>%
  inner_join(nrcLex) %>% 
  filter(sentiment %in% c("positive", "negative")) %>%
  count(day, sentiment) %>% 
  spread(sentiment, n, fill=0) %>% 
  mutate(sentiment = positive - negative) %>%
  mutate(method="nrc")

wf.nrc
```


```{r plot-all, echo=TRUE}
wf.aff_bin_nrc <- bind_rows(wf.affin, wf.bing, wf.nrc)
bind_rows(wf.aff_bin_nrc) %>%
  ggplot(aes(day, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```



```{r}

wf.aff_bin_nrc %>% 
  group_by(method) %>% 
  summarise(sentiment = sum(sentiment)) %>% 
  ggplot(aes(method, sentiment, fill = method)) +
  geom_bar(stat="identity") +  
  geom_text(aes(label=sentiment), position=position_dodge(width=0.9), vjust=-0.25)
```



```{r}
wf.aff_bin_nrc %>% 
  ggplot(aes(x=day, y=sentiment, group=method, color=method)) + 
  geom_line(size=1) + 
  geom_point() + labs(x="day", y="sentiment", title = "Sentiment vs Days for all 3 lexicons") + 
  scale_color_manual(values=c("red", "green", "blue"))
```



** Stock Market Data

```{r}

start<- as.Date("2020-03-15")
end <- as.Date("2020-04-27")


#Retrieving information on Top 5 Small Growth Funds as ranked by "U.S. News"
smallCapFunds <- c("PSGAX", "FKASX", "PGSGX", "QUASX", "TRSSX")
getSymbols(smallCapFunds, src = "yahoo", from = start, to = end)

#Retreiving information on Top 5 Mid Growth Funds as ranked by "U.S. News"
midCapFunds <- c("DFDMX", "CCSMX","PRDMX", "OTCAX", "BMGAX")
getSymbols(midCapFunds, src = "yahoo", from = start, to = end)

#Retrieving information on Top 5 Large Growth Funds as ranked by "U.s News"
largeCapFunds <- c("TRLGX", "PREFX", "TPLGX", "FDSVX", "PBLAX")
getSymbols(largeCapFunds, src = "yahoo", from = start, to = end)


#Retrieve Dow Jones Industrial Average
getSymbols("DJIA", src = "yahoo", from = start, to = end)

```



#Resource:  
  
* [https://bradleyboehmke.github.io/2015/12/scraping-html-text.html](https://bradleyboehmke.github.io/2015/12/scraping-html-text.html)  
* [Automated Data Collection PDF Book](Automated Data Collection PDF Book)
