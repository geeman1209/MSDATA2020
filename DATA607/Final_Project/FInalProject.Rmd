---
title: "Final Project"
author: "Devin Teran, Gabe Abreu, Amit Kapoor, Subhalaxmi Rout"
date: "4/27/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r cars}
library(rvest)
library(stringr)
library(purrr)
library(tidytext)
library(dplyr)
library(tidyr)
library(ggplot2)
```

```{r get_urls}
base_url <- 'https://www.whitehouse.gov/briefings-statements/remarks-president-trump-vice-president-pence-members-coronavirus-task-force-press-briefing-'

getPageURLs <- function(url) {
   add_number <- seq(2,33)
   urls <- str_c(base_url, add_number)
   return(urls)
}

urls <- getPageURLs(urls)
```

```{r get-data}
wh_briefings <- purrr::map(urls, ~read_html(.x) %>% html_nodes("p") %>% html_text())
 
```

```{r list-to-dataframe}
data <- data.frame(text=character(),
                   Day=integer(),
                 stringsAsFactors=FALSE)

for (i in 1:length(wh_briefings)){
  data <- rbind(data,cbind(as.data.frame(unlist(wh_briefings[[i]]),stringsAsFactors = FALSE),i))
}

colnames(data) <- c('text','day')
```

```{r create-stop-word-list}
custom_stop_words <- bind_rows(tibble(word = c("EDT"), 
                                      lexicon = c("custom")), 
                               stop_words)

custom_stop_words
```
```{r clean-data}
tidy_data <- data %>%
  mutate(linenumber = row_number()) %>%
  ungroup() %>%
  unnest_tokens(word, text)
```



```{r apply lexicon analysis}
afinn <- get_sentiments("afinn") 

tidy_data %>%
  anti_join(stop_words) %>%
  group_by(day) %>%
  inner_join(afinn) %>%
  count(word, sort = TRUE) %>%
  arrange(day,-n)
```



```{r affin, echo=TRUE}
affinLex <- get_sentiments("afinn")
wf.affin <- tidy_data %>% 
  anti_join(stop_words) %>% 
  group_by(day) %>%
  inner_join(affinLex) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method="affin")

wf.affin
```


```{r bing, echo=TRUE}
bingLex <- get_sentiments("bing")
wf.bing <- tidy_data %>% 
  anti_join(stop_words) %>% 
  group_by(day) %>%
  inner_join(bingLex) %>% 
  count(day, sentiment) %>% 
  spread(sentiment, n, fill=0) %>% 
  mutate(sentiment = positive - negative) %>%
  mutate(method="bing")

wf.bing
```

```{r nrc, echo=TRUE}
nrcLex <- get_sentiments("nrc")
wf.nrc <- tidy_data %>% 
  anti_join(stop_words) %>% 
  group_by(day) %>%
  inner_join(nrcLex) %>% 
  filter(sentiment %in% c("positive", "negative")) %>%
  count(day, sentiment) %>% 
  spread(sentiment, n, fill=0) %>% 
  mutate(sentiment = positive - negative) %>%
  mutate(method="nrc")

wf.nrc
```


```{r plot-all, echo=TRUE}
wf.aff_bin_nrc <- bind_rows(wf.affin, wf.bing, wf.nrc)
bind_rows(wf.aff_bin_nrc) %>%
  ggplot(aes(day, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```



```{r}

wf.aff_bin_nrc %>% 
  group_by(method) %>% 
  summarise(sentiment = sum(sentiment)) %>% 
  ggplot(aes(method, sentiment, fill = method)) +
  geom_bar(stat="identity") +  
  geom_text(aes(label=sentiment), position=position_dodge(width=0.9), vjust=-0.25)
```



```{r}
wf.aff_bin_nrc %>% 
  ggplot(aes(x=day, y=sentiment, group=method, color=method)) + 
  geom_line(size=1) + 
  geom_point() + labs(x="day", y="sentiment", title = "Sentiment vs Days for all 3 lexicons") + 
  scale_color_manual(values=c("red", "green", "blue"))
```










#Resource:  
  
* [https://bradleyboehmke.github.io/2015/12/scraping-html-text.html](https://bradleyboehmke.github.io/2015/12/scraping-html-text.html)  
* [Automated Data Collection PDF Book](Automated Data Collection PDF Book)
