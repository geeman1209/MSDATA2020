---
title: "Data 607 Final Project"
author: "Devin Teran, Gabe Abreu, Amit Kapoor, Subhalaxmi Rout"
date: "05/06/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Data source 2

### Twitter API

In this analysis, we use the twitter account of Donald Trump. All Twitter posts collected from Donald Trump’s twitter account `realDonaldTrump` post for corona pandemic. There are 15,000 tweets with #Coronavirus and #COVID19 between April 1 to April 30 in 2020 were extracted for analysis.

We use the data science software R with the tidyr,tidytext, dplyr packages to do text analytics and the twitteR package to connect and download data from twitter.

To collect data from twitter, we created one twitter application. This twitter data belongs to `realDonaldTrump` page. 
Followed the below steps to connect the app and download data from twitter. After getting the data, store the file in an excel file and do the sentiment analysis. 

+ go to apps.twitter.com
+ click create a new application, giving it a name and description
+ once you have created your app, copy the following info from the app homepage:
+ consumer key (API Key)
+ consumer secret (API Secret)
+ click the create my access token button, and copy
+ access token
+ access token secret

Load necessary packages and collect the data from twitter and store as a CSV file. 

```{r message=FALSE, warning=FALSE}
library(twitteR)
library(tidyr)

consumer_key <- "Lsk1wfKakbL6rfFC1dcLfpSrb"
consumer_secret <- "rKywBX5uJ3d13drP8NxgWGxC3TtPraKZiucS58J2OzFW0Rpcci"
access_token <- "1257449305616650243-J8E4TpO0Fmu6v5KXyY8T9owgSsPI7v"
access_secret <- "tgCzUd7fXgnSZFrLvmeMBYNJzH3i1hZkDX16PK3APkM3E"

#now lets connect
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)

# Collect 15000 tweets
trump_tweets <- twitteR::searchTwitter("#COVID-19 + #Coronavirus", n = 15000,lang = "en", since = "2020-04-01", until = "2020-04-30", retryOnRateLimit = 1e3)

trump_tweets_df <- dplyr::tbl_df(purrr::map_df(trump_tweets, as.data.frame))

# Write dataframe into an Excel
write.csv(trump_tweets_df, file = "trump_tweets.csv",row.names=FALSE)

# Read file from Github repository
trump_tweets_df <- utils::read.csv("https://raw.githubusercontent.com/geeman1209/MSDATA2020/master/DATA607/Final_Project/trump_tweets.csv",stringsAsFactors = FALSE)

# Raw data
head(trump_tweets_df)

# Remove http from statusSource
trump_tweets_df$statusSource <- gsub("<.*?>", "",trump_tweets_df$statusSource)

# Most favorited tweets
trump_fav <- trump_tweets_df %>%
  dplyr::arrange(desc(favoriteCount))

# Top 6 favorited tweets among the extracted 15,000 tweets
head(trump_fav)

# Most retweeted
trump_retweet <- trump_tweets_df %>%
  dplyr::arrange(desc(retweetCount)) %>%
  dplyr::distinct(text, .keep_all = TRUE)

# Top 6 retweeted texts among the extracted 15,000 tweets
head(trump_retweet)

trump_retweet_extracted <- trump_retweet[c(1,12,13,14)]

head(trump_retweet_extracted)
```

Data cleaning and tokenization

We will convert the data set into a corpus and then clean the corpus such as making all character lower case, remove punctuation marks, white spaces, and stop words. 

```{r fig.height=5, fig.width=6, message=FALSE, warning=FALSE}
library(tm)
library(textmineR)
library(RWeka)
library(wordcloud)
library(RColorBrewer)

trump_tweets_df_2 <- trump_tweets_df[c(1,2)]

# remove imocation from text
trump_tweets_df_2$text <- gsub("[^\x01-\x7F]", "", trump_tweets_df_2$text)


# Change dataset into a corpus
trump_tweets_corp <- tm::VCorpus(tm::VectorSource(trump_tweets_df_2))


# Data cleaning
trump_tweets_corp <- tm::tm_map(trump_tweets_corp, tolower)
trump_tweets_corp <- tm::tm_map(trump_tweets_corp, PlainTextDocument)
trump_tweets_corp <- tm::tm_map(trump_tweets_corp, removePunctuation)

# Remove stop words
new_stops <-c("covid","iphone","coronavirus","android","web","rt","chuonlinenews","Fashion", "fashionblogger", "Covid_19", "Juventus", "WuhanVirus","covid19","dranthonyfauci","scotgov youre", "rvawonk two")


trump_tweets_corp <- tm::tm_map(trump_tweets_corp, removeWords, words = c(stopwords("english"), new_stops))
trump_tweets_corp <- tm::tm_map(trump_tweets_corp, stripWhitespace)
trump_tweets_corp <- tm::tm_map(trump_tweets_corp, PlainTextDocument)
trump_tweets_corp <- tm::tm_map(trump_tweets_corp, removePunctuation)
trump_tweets_corp <- tm::tm_map(trump_tweets_corp, removeNumbers)


# Tokenize tweets texts into words
tokenizer <- function(x) {
  RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 2))
}

tdm <- TermDocumentMatrix(
  trump_tweets_corp,
  control = list(tokenize = tokenizer)
)

tdm <- as.matrix(tdm)
trump_tweets_cleaned_freq <- rowSums(tdm)


# Create a bi-gram (2-word) word cloud
pal <- RColorBrewer::brewer.pal(8,"Set1")
wordcloud::wordcloud(names(trump_tweets_cleaned_freq), trump_tweets_cleaned_freq, min.freq=50,max.words = 50, random.order=TRUE,random.color = TRUE, rot.per=.15, colors = pal,scale = c(3,1))
```

This word cloud shows the Word frequency of bi-grams(2 words). Based on the bi-gram we can know what most people are taking on Trump's post. 

### Sentiment Analysis 

Sentiment analysis helps us understand peoples’ feelings towards a specific subject. We will break the tweets’ sentences into words for further analysis.

```{r message=FALSE, warning=FALSE}
library(tibble)
library(tidytext)

# Transform sentences into words
trump_data <- trump_tweets_df %>%
  tidytext::unnest_tokens(output = "words", input = text, token = "words")

# Remove stop words from tibble
trump_clean_data <- trump_data %>%
  dplyr::anti_join(stop_words, by=c("words"="word")) %>% dplyr::filter(words != "trump" )
```

Polarity scores help us make quantitative judgments about the feelings of some text. In short, we categorize words from the tweets into positive and negative types and give them a score for analysis.
Then, we filter the dataset to get only words with a polarity score of 80 or more. I assigned the words with sentiment using `bing` lexicon and categorize words using polarity scores.

```{r message=FALSE, warning=FALSE}
library(tidyr)
library(ggplot2)

sentiment_data <- trump_clean_data %>% 
  # Inner join to bing lexicon by term = word
  dplyr::inner_join(get_sentiments("bing"), by = c("words" = "word")) %>% 
  # Count by term and sentiment, weighted by count
  dplyr::count(words, sentiment) %>%
  # Spread sentiment, using n as values
  tidyr::spread(sentiment, n, fill = 0) %>%
  # Mutate to add a polarity column
  dplyr::mutate(polarity = positive - negative)

# show summary of sentiment data
summary(sentiment_data)

polarity_data <- sentiment_data %>% 
  # Filter for absolute polarity at least 80 
  dplyr::filter(abs(polarity) >= 80) %>% 
  # add new column named as sentiments, shows positive/negative
  dplyr::mutate(
    Sentiments = ifelse(polarity > 0, "positive", "negative")
  )


ggplot2::ggplot(polarity_data, aes(reorder(words, polarity), polarity, fill = Sentiments)) +
  geom_col() + 
  ggtitle("Coronavirus tweets: Sentiment Word Frequency") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5, size = 10))+
  xlab("Word")
```

From the frequency of sentiments, we can see, negative sentiments have more frequency comparatively positive sentiments.

To get a clear picture of how positive and negative words are used, I assigned the words with a sentiment using the ‘bing’ lexicon and do a simple count to generate the top 10 most common positive and negative words used in the extracted tweets.

```{r message=FALSE, warning=FALSE}
word_counts <- trump_clean_data %>%
  #  sentiment analysis using the "bing" lexicon
  dplyr::inner_join(get_sentiments("bing"), by = c("words" = "word")) %>%
  # Count by word and sentiment
  dplyr::count(words, sentiment)


top_words <- word_counts %>%
  # Group by sentiment
 dplyr:: group_by(sentiment) %>%
  # Take the top 15 for each sentiment
  dplyr::top_n(15) %>%
  dplyr::ungroup() %>%
  # Make word a factor in order of n
  dplyr::mutate(words = reorder(words, n))



ggplot2::ggplot(top_words, aes(words, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = n, hjust=1), size = 3.5, color = "black") +
  facet_wrap(~sentiment, scales = "free") +  
  coord_flip() +
  ggtitle("Most common positive and negative words")


# Sentiment word cloud
tokenizer <- function(x) {
  RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 1, max = 1))
}

tdm <- TermDocumentMatrix(
  trump_tweets_corp,
  control = list(tokenize = tokenizer)
)

tdm <- as.matrix(tdm)
trump_tweets_cleaned_freq <- rowSums(tdm)


# Create a uni-gram (1-word) word cloud
pal <- RColorBrewer::brewer.pal(9,"Set2")
wordcloud::wordcloud(names(trump_tweets_cleaned_freq), trump_tweets_cleaned_freq, min.freq=50,max.words = 50, random.order=TRUE,random.color = TRUE, rot.per=.15, colors = pal,scale = c(3,1))
```

### Conclusion

+ Overall, the tweets convey an optimistic sentiment with the high frequency of words such as “Positive” and “improvement” of defeating Coronavirus.

+ When looking at unigram frequency (Coronavirus tweets: Sentiment Word Frequency graph), the word “death” stood out among the other words, which suggests that there are news or stories posted on twitter about people died in Covid-19 pandemic.

+ The most frequent words are related to Hospital Care, Died Hospital, Strict Lockdown suggesting that the Government is much more concerned about people and tries different ways to stop spreading the virus.


References

- Twitter API using R: https://www.youtube.com/watch?v=M_PnapGrpNI
- Twitter react to the Coronavirus pandemic: https://towardsdatascience.com/how-did-twitter-react-to-the-coronavirus-pandemic-2857592b449a


## Data Source 3

We collect data from Yahoo Finance site using `tidyquant` package. By default quantmod download and stores the symbols with their own names.

Here we will show the stock performance of S&P 500, Dow Jones30, NASDAQ, and Russel2000 from April 1 to April 30. 

We choose 3 levels of stocks such as High level, Medium Level, and Low level. We will select 5 companies from each category and will display their stocks from April 1 to April 30.

High level stocks companies:

+ Apple
+ Netflix
+ Amazon
+ Microsoft
+ Google

Medium level stocks companies:

+ Atlassian 
+ 4imprint Inc.
+ Etsy
+ Cirrus Logic
+ HotSpot

And, Low level stocks companies:

+ SolarWinds Corporation
+ Sturm, Ruger & Company, Inc.
+ Steven Madden, Ltd.
+ IPG Photonics Corporation
+ Winmark Corporation

Here we will see stock performace of S&P 500, Dow30, NASDAQ and Russel2000 from April 1 to April 30.

Load necessary libraries.

### Stock performance of S&P 500, Dow30, NASDAQ and Russel 2000

```{r message=FALSE, warning=FALSE}
library(tidyquant)
library(DT)
library(dplyr)
library(ggplot2)

tickers = c( "^GSPC", "^DJI","^IXIC", "^RUT")

prices <- tq_get(tickers,
           from = "2020-04-01",
           to = "2020-04-30",
           get = "stock.prices")

prices %>%
  dplyr::group_by(symbol) %>%
  slice(1)

prices %>%
  ggplot(aes(x = date, y = adjusted, color = symbol)) +
  geom_line()

DT::datatable(prices)

prices %>%
  ggplot(aes(x = date, y = adjusted, color = symbol)) +
  geom_line() +
  facet_wrap(~symbol,scales = 'free_y') +
  theme_classic() +
  labs(x = 'Date',
       y = "Adjusted Price",
       title = "Price Chart") +
  scale_x_date(date_breaks = "week",
               date_labels = "%b %d")

```

###  Stock performance of Apple, Netflix, Amazon, Microsoft, and Google

```{r message=FALSE, warning=FALSE}
tickers = c("AAPL", "NFLX", "AMZN", "MSFT", "GOOG")

prices_hi <- tq_get(tickers,
           from = "2020-04-01",
           to = "2020-04-30",
           get = "stock.prices")

prices_hi %>%
  dplyr::group_by(symbol) %>%
  slice(1)

prices_hi %>%
  ggplot(aes(x = date, y = adjusted, color = symbol)) +
  geom_line()

DT::datatable(prices_hi)

prices_hi %>%
  ggplot(aes(x = date, y = adjusted, color = symbol)) +
  geom_line() +
  facet_wrap(~symbol,scales = 'free_y') +
  theme_classic() +
  labs(x = 'Date',
       y = "Adjusted Price",
       title = "Price Chart") +
  scale_x_date(date_breaks = "week",
               date_labels = "%b %d")
```

### Stock performance of Atlassian, 4imprint Inc , Etsy, Cirrus Logic, and HotSpot 

```{r message=FALSE, warning=FALSE}
tickers = c("TEAM", "FOUR.L", "ETSY", "CRUS", "HUBS")

prices_med <- tq_get(tickers,
           from = "2020-04-01",
           to = "2020-04-30",
           get = "stock.prices")

prices_med %>%
  dplyr::group_by(symbol) %>%
  slice(1)

prices_med %>%
  ggplot(aes(x = date, y = adjusted, color = symbol)) +
  geom_line()

DT::datatable(prices_med)

prices_med %>%
  ggplot(aes(x = date, y = adjusted, color = symbol)) +
  geom_line() +
  facet_wrap(~symbol,scales = 'free_y') +
  theme_classic() +
  labs(x = 'Date',
       y = "Adjusted Price",
       title = "Price Chart") +
  scale_x_date(date_breaks = "week",
               date_labels = "%b %d")
```

### Stock performance of SolarWinds Corporation, Sturm Ruger & Company , Steven Madden, IPG Photonics Corporation, and Winmark Corporation

```{r message=FALSE, warning=FALSE}
tickers = c("SWI", "RGR", "SHOO", "IPGP", "WINA")

prices_low <- tq_get(tickers,
           from = "2020-04-01",
           to = "2020-04-30",
           get = "stock.prices")

prices_low %>%
  dplyr::group_by(symbol) %>%
  slice(1)

prices_low %>%
  ggplot(aes(x = date, y = adjusted, color = symbol)) +
  geom_line()

DT::datatable(prices_low)

prices_low %>%
  ggplot(aes(x = date, y = adjusted, color = symbol)) +
  geom_line() +
  facet_wrap(~symbol,scales = 'free_y') +
  theme_classic() +
  labs(x = 'Date',
       y = "Adjusted Price",
       title = "Price Chart") +
  scale_x_date(date_breaks = "week",
               date_labels = "%b %d")
```

### Conclusion

In April, above mentioned all stock has fluctuated. One common pattern we can say 1st and 2nd week of April stocks are doing good then in the 3rd week drastically falls down and in the 4th week again going up. 

Reference: https://www.codingfinance.com/post/2018-03-27-download-price/
