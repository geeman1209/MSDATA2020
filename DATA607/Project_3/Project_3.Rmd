---
title: "Project 3: Valuable Data Scientist Skills"
author: "Team Emerald Orcas - Gabe Abreu, Amit Kapoor & Devin Teran "
date: "3/17/2020"
output:
  html_document:
    highlight: pygments
    theme: cerulean
  pdf_document: default
---

```{r setup, include=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(RCurl)
library(stringr)
library(pdp)
library(gridExtra)
library(dbConnect)
knitr::opts_chunk$set(echo = TRUE)
```

# Overview  
The purpose of Project 3 is to use data to answer the question, ***"Which are the most valued data science skills?".*** 

After initial brainstorming, we determined to use the following dataset to perform analysis: https://www.kaggle.com/elroyggj/indeed-dataset-data-scientistanalystengineer/kernels.

The dataset takes data related job postings and breaks them down by industry, company, skills, job titles, location and categorical job types (data scientist, data analyst, and data engineer). We determined this is an appropriate dataset since it is employers that generally determine what are valued skill sets in an occupation.  
<br />  
There are important but subtle differences between the three major job titles within the data science job family.

***Data Analysts*** query and process data, provide reports, summarize and visualize data. They generally have a strong grasp of how to utilize existing tools and methods to solve problems, and help their respective company understand specific problems. Typical tasks performed by analysts are: cleaning and organizing raw data, use statistics to gain a big picture perspective on their data, find trends in data, and create visualizations for company staff to interpret the data.

***Data Engineers*** are the professionals who prepare and create the infrastructure for "big data". They transform the data into a useable format for analysis by data scientists. Data Engineers skill set lean toward the software development skill set. They build APIs for data consumption, integrate various datasets into existing data pipelines, monitor and test data pipelines to ensure optimal performance.

***Data Scientists*** apply statistics, machine learning, and analytic approaches to solve problems. They deep dive into big data, unstructured data, and regular data to find patterns and future trends. Data Scientists are expected to have programming skills and an ability to design new algorithms. They uncover hidden trends by using supervised and unsupervised learning methods toward their machine learning models.Some of a data scientist's task include: using statistical models to determine the validity of analyses, use machine learning to create better predictive algorithms, test and improve their machine learnig models and create data visualizations to summarize advanced analysis. 

# Data Import  

```{r}
#Create connection to database and load acquired data into data frame.
username = 'admin'
password = 'abcd1234'
dbname = 'ds_skills'
host = 'dsdb-1.cswpiojk9qjg.us-east-1.rds.amazonaws.com'
myDb = dbConnect(MySQL(), user=username, password=password, dbname=dbname, host=host)
```

```{r}
#Import indeed dataset
url <- getURL("https://raw.githubusercontent.com/geeman1209/MSDATA2020/master/DATA607/Project_3/indeed_job_dataset.csv")

raw_data <- read.csv(text = url)
```
# Cleaning the Data  

#### Let's take a look at our raw data:
```{r}
glimpse(raw_data)
```

We're going to get rid of columns that are not necessary for our analysis.  This includes all state data and information related to the company that posting the job.  


```{r clean-data}
df_1 <- raw_data
drops <- c("X","Link","Company","No_of_Reviews","No_of_Stars","Date_Since_Posted","Company_Revenue","Company_Employees", "Description", "Location")
df_1 <- select(df_1 ,-c(all_of(drops)))

#delete state related data in the columns 17-27
df_2 <- select(df_1, -c(17:27))
```
  
The raw data originally had 6 columns, one for each industry. If the job posting was within that industry, the column would be set to 1 if not, it was set to 0. We decided to combined these 6 columns into a single column 'Industry' with 6 different categories: Consulting&Business, Internet&Software, Banks&FiancialServices, HealthCare, Insurance and Other.  

Additionally, we wanted to distinguish if the job titles had any variations of the words 'junior' or 'senior' in the title.  Both of these additions will make our analysis easier.


```{r calculate-additional-columns}
#single column for industry
df_2['Industry'] <- ifelse(df_2$Consulting.and.Business.Services==1,'Consulting&Business',ifelse(df_2$Internet.and.Software,'Internet&Software',ifelse(df_2$Banks.and.Financial.Services==1,'Banks&FiancialServices',ifelse(df_2$Health.Care==1,'HealthCare',ifelse(df_2$Insurance==1,'Insurance',ifelse(df_2$Other_industries==1,'Other',NA))))))

#flag junior or senior job titles
df_2['Level'] <- ifelse(grepl("Jr|Junior",ignore.case=TRUE,df_2$Job_Title),'Junior',ifelse(grepl("Sr|Senior",ignore.case=TRUE,df_2$Job_Title),"Senior","Other"))
```  
  
# Data Analysis     
  
#### Let's look at the Most Frequently Seen Skills  
```{r skill-count, echo=TRUE}
leadingtech <- c("python","sql","machine.learning","r","hadoop","tableau","sas","spark","java","Others")

skill_ct <- raw_data %>% 
    select(all_of(leadingtech)) %>%
    gather(key,value) %>% 
    filter(value==1) %>%
    group_by(key) %>% 
    summarise(n = n())

skill_ct  
```  
  
#### What are the most popular skills for a data scientist? Do these top skils differ for data analysts and data engineers?     
  
The data shows that **data engineers** are typically expected to know Java, Hadoop, Spark, SQL, and python. A lot of the "other" skills required of data engineers are cloud based technologies (Azure, AWS, Google Cloud) and linux. This is to be expected, as previously mentioned, data engineer's skill set gravitates toward software development, a field where Java and SQL is prevelant for backend systems. Linux since it is open sourced and flexible is the operating system of choice, unless you're in a Microsoft-based environment.

**Data analysts** are exptected to know Tableau to a greater degree than data scientists or engineers. Data analysts are also required to know SQL, to perform basic queries from historical data.  Tableau is an interactive data visualization tool, also known as a business intelligence tool.  SAS is another popular skill set for analysts, is statistical software used for analysis. These skills match well with the provided job description for analysts. Their skills are going to be geared for visualization, mining past data, and created business reports.

**Data scientist**, as the data shows, is a versatile role. However, Data scientists are expected know Machine Learning and R by significant margins above engineers and analysts. Data Scientists, to a lesser degree, are also expected to know Hadoop, Spark, and Java. Theses are the key skill sets for engineers. We suspect, a succesful data scientist may not be an expert in data engineering but should at least be familiar with the concepts associated with the profession. Of course, python is univerally required all positions.   
  
```{r most-common-skills}
skills <- df_2 %>% group_by(Job_Type) %>% summarise(sum(python),sum(sql),sum(machine.learning),sum(r),sum(hadoop),sum(tableau),sum(sas),sum(spark),sum(java),sum(Others))
colnames(skills) <- c('Job_Type','Python','SQL','Machine_Learning','R','Hadoop','Tableau','SAS','Spark','Java','Others')
skills <- pivot_longer(skills,cols=c(2:11),names_to = "Skill",values_to = "Count")

job_type_count <- df_2 %>% group_by(Job_Type) %>% tally()
a <- skills %>% inner_join(job_type_count,by="Job_Type")
skills['Total_Jobs']  <- a$n
skills['Perct_Total'] <- a$Count / a$n
  
ggplot(skills, aes(x=reorder(Skill,-Perct_Total),y=Perct_Total,fill=Job_Type)) +
  geom_bar(stat="identity", position = 'dodge') +
  xlab('Skill') +
  ylab('% of Total Postings that List this Skill') + 
  ggtitle('Top Skills on Job Listings - LinkedIn') +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text.x = element_text(angle = 90)) 
```  
  
#### Do Skills Differ Among Junior vs. Senior Data Scientists?  

There isn't a difference in the top skills required from a junior or senior data scientist. Data scientists, senior and junior, are expected to know machine learning, python, and R. As you go down the list, there are differences in required skills. Senior data scientists are expected to know more data engineering (Hadoop, Spark, SQL) while junior level scientists, visualization tools like Tableau are in higher demand. 

```{r data-science-skills}
ds <- filter(df_2,Job_Type == 'data_scientist',Level %in% c("Senior","Junior"))
ds_skills <- ds %>% group_by(Level) %>% summarise(sum(python),sum(sql),sum(machine.learning),sum(r),sum(hadoop),sum(tableau),sum(sas),sum(spark),sum(java),sum(Others))
colnames(ds_skills) <- c('Level','Python','SQL','Machine_Learning','R','Hadoop','Tableau','SAS','Spark','Java','Others')
ds_skills_long <- pivot_longer(ds_skills,cols=c(2:11),names_to = "Skill",values_to = "Count")

ds_job_level<- df_2 %>% group_by(Level) %>% tally()
b <- ds_skills_long %>% inner_join(ds_job_level,by="Level")
ds_skills_long['Total_Jobs']  <- b$n
ds_skills_long['Perct_Total'] <- b$Count / b$n

senior <- filter(ds_skills_long,ds_skills_long$Level == "Senior")
s <- ggplot(senior, aes(x=reorder(Skill,Perct_Total),y=Perct_Total,fill=Level)) +
  geom_bar(stat="identity", position = 'dodge') +
  coord_flip() +
  xlab('Skill') +
  ylab('') + 
  ggtitle('Senior Level') +
  theme(plot.title = element_text(hjust = 0.5))

junior <- filter(ds_skills_long,ds_skills_long$Level == "Junior")
j <- ggplot(junior, aes(x=reorder(Skill,Perct_Total),y=Perct_Total,fill=Level)) +
  geom_bar(stat="identity", position = 'dodge') +
  coord_flip() +
  xlab('Skill') +
  ylab('% Postings with Skill') + 
  ggtitle('Junior Level') +
  theme(plot.title = element_text(hjust = 0.5))

grid.arrange(s,j,nrow=2,top = "Top Skills for Junior vs. Senior Data Scientists")

```
  
#### Which industries have the most job postings for data scientists?  

Top industries are consulting/business, internet/software, banking/finance, healthcare and insurance.

```{r count-job-posting-by-industry}
posts_by_industry <- filter(df_2,Job_Type == 'data_scientist',is.na(df_2$Industry) == FALSE)
industry_count <- posts_by_industry %>% group_by(Industry) %>% tally()

colnames(industry_count) <- c('Industry','Count')

ggplot(industry_count, aes(x=reorder(Industry,Count),y=Count)) +
  geom_bar(stat="identity", position = 'dodge') +
  coord_flip() +
  xlab('Industry') +
  ylab('Count of Job Postings') + 
  ggtitle('Top Industries Among Data Job Listings - LinkedIn') +
  theme(plot.title = element_text(hjust = 0.5))
```
  
#### Let's take the top skills to see which pay the highest salary for data scientists?

The two extremes of the spectrum require little machine learning, python, and R. This actually makes sense if the professionals making above $160K are in management, so they're not actively coding as much as senior-mid tier professionals. The data shows machine learning, python, and R are the highest paying skills.

```{r avg-highest-salary}
ds_salary <- select(filter(df_2,df_2$Job_Type=='data_scientist'), Queried_Salary,python,sql,machine.learning,r)
ds_salary_job_type <- pivot_longer(ds_salary,cols=c(2:5),names_to='Skill')
skill <- ds_salary_job_type %>% group_by(Queried_Salary,Skill) %>% summarise(sum(value))
colnames(skill) <- c('Salary_Range','Skill','Count')

ggplot(skill, aes(x=Salary_Range,y=Count,fill=Skill)) +
  geom_bar(stat="identity", position = 'dodge')+
  xlab('Salary Range') +
  ylab('Count of Postings') +
  theme(axis.text.x = element_text(angle = 90)) + 
  ggtitle("Highest Paying Skills") +
  theme(plot.title = element_text(hjust = 0.5)) 
```
  
#### Do certain industries require certain skills? We're going to ignore OTHER since it's overwhelmingly popular and we don't know which industries it represents.

The required skills are similiar across industries most in-demand of data scientists. This is a boon to all data scientists as moving across industries should be easier than other professions. This opens the job market in a positive manner to all current and future data scientists.

```{r industry-skills}
#remove rows where industry is NA or OTHER
industry <- filter(df_2,Job_Type == 'data_scientist',df_2$Industry != 'Other',is.na(df_2$Industry) == FALSE)
industry_skills <- industry %>% group_by(Industry) %>% summarise(sum(python),sum(sql),sum(machine.learning),sum(r),sum(hadoop),sum(tableau),sum(sas),sum(spark),sum(java),sum(Others))

colnames(industry_skills) <- c('Industry','Python','SQL','Machine_Learning','R','Hadoop','Tableau','SAS','Spark','Java','Others')

industry_skills <- pivot_longer(industry_skills,cols=c(2:11),names_to = "Skill",values_to = "Count")

#Internet & Software
internet <- filter(industry_skills,industry_skills$Industry %in% c('Internet&Software','Consulting&Business'))
a <- ggplot(internet, aes(x=reorder(Skill,-Count),y=Count,fill=Industry)) +
  geom_bar(stat="identity", position = 'dodge')+
  xlab('') +
  ylab('Count of Job Posts') +
  theme(axis.text.x = element_text(angle = 90)) +
  expand_limits(y = c(0, 400)) +
  theme(legend.position = c(0.6, 0.8))

#Healthcare, Insurance & Banks
healthcare_insurance_banks <- filter(industry_skills,industry_skills$Industry %in% c('HealthCare','Insurance','Banks&FinancialServices'))
b <- ggplot(healthcare_insurance_banks, aes(x=reorder(Skill,-Count),y=Count,fill=Industry)) +
  geom_bar(stat="identity", position = 'dodge')+
  xlab('Skill') +
  ylab('Count of Job Posts') +
  theme(axis.text.x = element_text(angle = 90)) +
  expand_limits(y = c(0,160)) + 
  theme(legend.position = c(0.6725, 0.8))

grid.arrange(a,b,nrow=1,top = "Top Skills Across Industry for Data Scientist Job Postings",widths=c(1,1))

```


#### Additional Visualizations

```{r polar-split, echo=TRUE,warning=FALSE}
# gather() takes leadingtech as multiple columns and gathers them into key-value pairs to make “wide” data longer
# group_by_ groups the data by leadingtech
# summarise the results by leadingtech counts for all 3 job types data_analyst, data_engineer and data_scientist
# draw polygon for skillset and its count by Job_Type
# highlights the count points
# split up by Job_Type using reformulate to have splitted horizontally
# draw with polar coordinates
# set the theme

polar_split <- raw_data %>% 
  gather(key, value, all_of(leadingtech)) %>% 
  group_by_("Job_Type", "key")  %>% 
  summarise(count = sum(value)) %>% 
  ggplot() + 
  geom_polygon(aes_string(x="key", y="count", col="Job_Type", group="Job_Type"), size=0.5, fill=NA) + 
  geom_point(aes_string(x="key", y="count", col="Job_Type", group="Job_Type"), size=1) + 
  facet_grid(reformulate("Job_Type")) + 
  coord_polar() + 
  theme_minimal() +
  theme(legend.position = "top",
                 legend.title = element_blank(), 
                 legend.spacing.x = grid::unit(0.25, 'cm'),
                 legend.text = element_text(margin = margin(t = 10),vjust=2, size=15),
                 axis.title = element_blank(),
                 axis.text.x = element_text(size=10),
                 axis.text.y = element_blank())

polar_split

```

### Conclusion:
Since we used a curated dataset, our analysis was restricted to the quality of the raw dataset.  One thing that was limiting, was that the salary information was listed as categorical data in ranges (<$80,000,$80,000-$99,999, etc.).  These ranges are pretty large and it would've been nicer to have salary be a contious variable, so analysis could be more detailed.



### References: 

https://medium.com/optima-blog/using-polar-coordinates-for-better-visualization-1d337b6c9dec
https://www.kaggle.com/inigoml/data-exploration




